{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 12 GPT4ALL로 로컬 모델 실행하기\n",
    "\n",
    "- **GPT4ALL**은 코드, 채팅 형식의 대화를 포함한 방대한 양의 데이터로 학습된 오픈소스 챗봇 생태계\n",
    "- LangChain을 사용하여 GPT4ALL 모델과 상호작용하는 방법 설명"
   ],
   "id": "13865112fc26ad36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**모델 다운로드**\n",
    "\n",
    "![](images/gpt4all_models.png)\n",
    "\n",
    "[gpt4all 페이지](https://gpt4all.io/index.html)에는 `Model Explorer` 섹션이 있습니다.\n",
    "(더 많은 정보를 원하시면 https://github.com/nomic-ai/gpt4all 을 방문하세요.)\n",
    "\n",
    "1. [공식 홈페이지](https://gpt4all.io/index.html) 에서 다운로드 가능한 모델을 다운로드 받습니다. 본인의 PC 사양에서 구동가능한 모델을 선택하는 것이 좋습니다.\n",
    "2. 본 튜토리얼에서는 `EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf`(10.69GB) 모델을 다운로드 받아 진행하겠습니다.\n",
    "3. 다운로드 받은 모델은 `models` 폴더 생성 후 해당 폴더에 다운로드 받습니다."
   ],
   "id": "a9caf0ed3deb7514"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- `local_path` 변수에 로컬 파일 경로(`\"./models/EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf\"`)를 할당\n",
    "- 이 경로는 사용자가 원하는 로컬 파일 경로로 대체 가능"
   ],
   "id": "cbeb04cbe0a19d24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "local_path = \"models/EEVE-Korean-Instruct-10.8B-v1.0-Q4_0.gguf\"",
   "id": "7dbe7aa84e07166e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**모델 정보 설정**\n",
    "\n",
    "- 로컬에서 실행하려면 호환되는 ggml 형식의 모델을 다운로드\n",
    "  - 관심 있는 모델을 선택\n",
    "  - UI를 사용하여 다운로드하고 `.bin` 파일을 `local_path`(아래 참고)로 이동"
   ],
   "id": "be3edd266941040b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 프롬프트\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
    "<s>Human: {question}</s>\n",
    "<s>Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# GPT4All 언어 모델 초기화\n",
    "# model는 GPT4All 모델 파일의 경로를 지정\n",
    "llm = GPT4All(\n",
    "    model=local_path,\n",
    "    backend=\"gpu\",  # GPU 설정\n",
    "    streaming=True,  # 스트리밍 설정\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # 콜백 설정\n",
    ")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 질의 실행\n",
    "response = chain.invoke({\"question\": \"대한민국의 수도는 어디인가요?\"})\n"
   ],
   "id": "12fd2ca0b12e2d3a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
