{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hugging Face Inference API 활용하기\n",
    "\n",
    "- **Hugging Face**는 무료 모델과 데이터셋을 공유 가능한 오픈소스 플랫폼\n",
    "- 다양한 머신러닝 애플리케이션을 구축하고 배포할 수 있으며, 이를 위한 엔드포인트 서비스도 제공\n",
    "- **엔드포인트**란 사용자가 원격에서 머신러닝 모델을 호출하고 사용할 수 있도록 설정된 URL 또는 API 인터페이스\n",
    "- 서버리스 엔드포인트와 Inference API는 이러한 원격 호출과 배포를 간소화해 주는 서비스\n",
    "- **서버리스 엔드포인트**는 사용자가 직접 관리할 필요 없이 모델을 호스팅하고 자동 확장 할 수 있는 API\n",
    "- **Inference API**는 Hugging Face에서 제공하는 사전 학습된 모델을 쉽게 사용할 수 있도록 하는 API 서비스"
   ],
   "id": "19b7a34ace0d80e3"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-10T16:31:05.077984Z",
     "start_time": "2026-02-10T16:22:28.245601Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "load_dotenv()\n",
    "logging.langsmith(\"CH04-Models\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH04-Models\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 모델 웹 페이지의 Chat Format에서 모델의 프롬프트 양식과 예시를 확인한다\n",
    "- 모델마다 학습된 프롬프트 양식이 다르므로 정해진 양식을 사용하면 모델이 기대하는 형식에 맞춰 정확한 응답을 얻을 수 있다"
   ],
   "id": "4143d025ab51c561"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "<|system|>\n",
    "You are a helpful assistant.<|end|>\n",
    "<|user|>\n",
    "Question?<|end|>\n",
    "<|assistant|>\n",
    "```"
   ],
   "id": "2c26458eb6d019e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:31:05.078453Z",
     "start_time": "2026-02-10T16:22:28.349754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "You are a helpful assistant.<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ],
   "id": "4d4fbc95f022eed9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Serverless Endpoints**\n",
    "\n",
    "- Inference API 는 무료로 사용할 수 있으며 요금은 제한\n",
    "- 프로덕션을 위한 추론 솔루션이 필요한 경우, [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) 서비스를 확인\n",
    "- Inference Endpoints 를 사용하면 모든 머신 러닝 모델을 전용 및 완전 관리형 인프라에 손쉽게 배포 가능.\n",
    "- 클라우드, 지역, 컴퓨팅 인스턴스, 자동 확장 범위 및 보안 수준을 선택하여 모델, 지연 시간, 처리량 및 규정 준수 요구 사항에 맞게 설정\n",
    "- 다음은 Inference API 에 액세스하는 방법의 예시\n",
    "- 참고\n",
    "  - [Serverless Endpoints](https://huggingface.co/docs/api-inference/index)\n",
    "  - [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)\n"
   ],
   "id": "edef1b59bb421547"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- `repo_id` 변수에 HuggingFace 모델의 `repo ID`(저장소 ID) 를 할당\n",
    "  - `microsoft/Phi-3-mini-4k-instruct` 모델: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"
   ],
   "id": "913f4f8612c6f98a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# 사용할 모델의 저장소 ID를 설정합니다.\n",
    "repo_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,  # 모델 저장소 ID를 지정합니다.\n",
    "    max_new_tokens=256,  # 생성할 최대 토큰 길이를 설정합니다.\n",
    "    temperature=0.1,\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # 허깅페이스 토큰\n",
    ")\n",
    "\n",
    "# LLMChain을 초기화하고 프롬프트와 언어 모델을 전달합니다.\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "# 질문을 전달하여 LLMChain을 실행하고 결과를 출력합니다.\n",
    "response = chain.invoke({\"question\": \"what is the capital of South Korea?\"})\n",
    "print(response)\n"
   ],
   "id": "eaee108492e44e58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dedicated Inference Endpoint로 원격 호스팅하기\n",
    "\n",
    "- 앞서 살펴본 서버리스 엔드포인트와 Inference API의 고급 버전\n",
    "- 더 높은 성능과 안정성이 필요하다면 전용 서버 환경에서 모델을 직접 호스팅하는 **Dedicated Inference Endpoint** 고려"
   ],
   "id": "f10174766a39ae87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- `hf_endpoint_url` 변수에 Inference Endpoint의 URL을 설정",
   "id": "2f9c34b21ea6f1e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:33:49.675281Z",
     "start_time": "2026-02-10T16:33:49.672290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inference Endpoint URL을 아래에 설정합니다.\n",
    "hf_endpoint_url = \"https://slcalzucia3n7y3g.us-east-1.aws.endpoints.huggingface.cloud\""
   ],
   "id": "2497ded3c3d470eb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:33:56.965655Z",
     "start_time": "2026-02-10T16:33:56.898927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    # 엔드포인트 URL을 설정합니다.\n",
    "    endpoint_url=hf_endpoint_url,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.01,\n",
    "    task=\"text-generation\",\n",
    ")\n",
    "\n",
    "# 주어진 프롬프트에 대해 언어 모델을 실행합니다.\n",
    "llm.invoke(input=\"대한민국의 수도는 어디인가요?\")"
   ],
   "id": "e913606fa2ec1c11",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InferenceClient' object has no attribute 'post'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      1\u001B[39m llm = HuggingFaceEndpoint(\n\u001B[32m      2\u001B[39m     \u001B[38;5;66;03m# 엔드포인트 URL을 설정합니다.\u001B[39;00m\n\u001B[32m      3\u001B[39m     endpoint_url=hf_endpoint_url,\n\u001B[32m   (...)\u001B[39m\u001B[32m      6\u001B[39m     task=\u001B[33m\"\u001B[39m\u001B[33mtext-generation\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      7\u001B[39m )\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# 주어진 프롬프트에 대해 언어 모델을 실행합니다.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m대한민국의 수도는 어디인가요?\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-1B5rchLC-py3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:390\u001B[39m, in \u001B[36mBaseLLM.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    380\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    381\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    382\u001B[39m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[32m   (...)\u001B[39m\u001B[32m    386\u001B[39m     **kwargs: Any,\n\u001B[32m    387\u001B[39m ) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    388\u001B[39m     config = ensure_config(config)\n\u001B[32m    389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m--> \u001B[39m\u001B[32m390\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    391\u001B[39m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    392\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    393\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    394\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    395\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    396\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    397\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    398\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    399\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    400\u001B[39m         .generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m]\n\u001B[32m    401\u001B[39m         .text\n\u001B[32m    402\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-1B5rchLC-py3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:763\u001B[39m, in \u001B[36mBaseLLM.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    755\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m    756\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    757\u001B[39m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[32m   (...)\u001B[39m\u001B[32m    760\u001B[39m     **kwargs: Any,\n\u001B[32m    761\u001B[39m ) -> LLMResult:\n\u001B[32m    762\u001B[39m     prompt_strings = [p.to_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m763\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-1B5rchLC-py3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:966\u001B[39m, in \u001B[36mBaseLLM.generate\u001B[39m\u001B[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    951\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m.cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[32m    952\u001B[39m     run_managers = [\n\u001B[32m    953\u001B[39m         callback_manager.on_llm_start(\n\u001B[32m    954\u001B[39m             \u001B[38;5;28mself\u001B[39m._serialized,\n\u001B[32m   (...)\u001B[39m\u001B[32m    964\u001B[39m         )\n\u001B[32m    965\u001B[39m     ]\n\u001B[32m--> \u001B[39m\u001B[32m966\u001B[39m     output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    967\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    968\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    969\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[32m    970\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) > \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-1B5rchLC-py3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:787\u001B[39m, in \u001B[36mBaseLLM._generate_helper\u001B[39m\u001B[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[39m\n\u001B[32m    777\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_generate_helper\u001B[39m(\n\u001B[32m    778\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    779\u001B[39m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m    783\u001B[39m     **kwargs: Any,\n\u001B[32m    784\u001B[39m ) -> LLMResult:\n\u001B[32m    785\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    786\u001B[39m         output = (\n\u001B[32m--> \u001B[39m\u001B[32m787\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    788\u001B[39m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    789\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    790\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[32m    791\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    792\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    793\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    794\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    795\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._generate(prompts, stop=stop)\n\u001B[32m    796\u001B[39m         )\n\u001B[32m    797\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    798\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-1B5rchLC-py3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1526\u001B[39m, in \u001B[36mLLM._generate\u001B[39m\u001B[34m(self, prompts, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1523\u001B[39m new_arg_supported = inspect.signature(\u001B[38;5;28mself\u001B[39m._call).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1524\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[32m   1525\u001B[39m     text = (\n\u001B[32m-> \u001B[39m\u001B[32m1526\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1527\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m   1528\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(prompt, stop=stop, **kwargs)\n\u001B[32m   1529\u001B[39m     )\n\u001B[32m   1530\u001B[39m     generations.append([Generation(text=text)])\n\u001B[32m   1531\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations=generations)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-1B5rchLC-py3.11/lib/python3.11/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:312\u001B[39m, in \u001B[36mHuggingFaceEndpoint._call\u001B[39m\u001B[34m(self, prompt, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    308\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    309\u001B[39m     invocation_params[\u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m] = invocation_params[\n\u001B[32m    310\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mstop_sequences\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    311\u001B[39m     ]  \u001B[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m312\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpost\u001B[49m(\n\u001B[32m    313\u001B[39m         json={\u001B[33m\"\u001B[39m\u001B[33minputs\u001B[39m\u001B[33m\"\u001B[39m: prompt, \u001B[33m\"\u001B[39m\u001B[33mparameters\u001B[39m\u001B[33m\"\u001B[39m: invocation_params},\n\u001B[32m    314\u001B[39m         stream=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    315\u001B[39m         task=\u001B[38;5;28mself\u001B[39m.task,\n\u001B[32m    316\u001B[39m     )\n\u001B[32m    317\u001B[39m     response_text = json.loads(response.decode())[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mgenerated_text\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    319\u001B[39m     \u001B[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001B[39;00m\n\u001B[32m    320\u001B[39m     \u001B[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001B[39;00m\n",
      "\u001B[31mAttributeError\u001B[39m: 'InferenceClient' object has no attribute 'post'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T16:34:57.837059Z",
     "start_time": "2026-02-10T16:34:57.831764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "            \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n",
    "        ),\n",
    "        (\"user\", \"Human: {question}\\nAssistant: \"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ],
   "id": "ca09a35f3eca279e",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
