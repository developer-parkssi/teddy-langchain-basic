## Chapter02 RAG의 기막힌 능력

- RAG 도입을 주저하는 이유는 주로 복잡해 보이는 용어와 기술적 장벽. 그러나 기본 원리만 이해한다면 생각보다 어려운 기술 X

### 플러그인처럼 교체하는 방식의 쉬운 구현

- 완전 파인 튜닝: 전문적인 내용이기에 개인이 수행하기에 어려움
- PEFT(Parameter-Efficient Fine-Tuning): 사전 학습된 모델의 일부 파라미터만 미세 조정하는 방법, 충분한 시간과 노력을 들인다면 개인적으로 시도해볼만함
- 책에 나온 그림을 보면 `Prompt Engineering`(제일 쉬움)에 비해 난이도가 높지 않다

### 최신 정보를 기반으로 답변

- RAG은 최신 정보에 대한 답변에서도 월등히 뛰어난 성능. 사용자가 원하는 최신의 문서를 DB에 넣어 업데이트 가능하기 때문

### 답변 과정을 투명하게 확인 및 해석 가능

- RAG은 전 과정을 모니터링하고 추정 가능하며, 각 단계를 상세히 분석하고 조정할 수 있다
- 책 그래프를 보면 RAG는 답변의 투명성과 해석 능력 측면에서 압도적인 성능
- LLM의 동작 과정을 추적하는 도구인 LangSmith에서 질문을 던지면 왼쪽 추적 과정에서 답변 도출까지의 세부과정, 소요 시간, 토큰 수 등을 확인 가능
- 추후 더 복잡한 설계를 진행해도 심도있게 분석 가능
- 실제로 **Retriever**를 클릭하면 질문이 입력되었을 때 어떤 문서의 어느 구절에서 관련 내용을 검색했는지를 상세하게 확인 가능

**할루시네이션 감소**

- RAG은 LLM의 대표적 부작용인 할루시네이션을 감소시키는 데에도 효과적
- 유요한 정보만을 기반으로 답변을 도출하도록 강제하거나, 주어진 문서에서만 답변의 출처를 찾도록 하는 방식으로 오류를 줄일 수 있기 때문

## 3. LangChain을 이용한 RAG 시스템 구축

- **LangChain**은 대규모 언어 모델로 구동되는 애플리케이션을 개발하기 위한 프레임워크
- GPT와 같은 언어 모델과 우리가 만들고자 하는 서비스나 프로세스를 쉽게 연결해 주는 도구
- GPT로 어떤 문서에 기반한 Q&A 시스템을 구축하고자 할 때, 문서 안에 있는 텍스트를 읽어들일 수 있게끔 별도로 작업을 해줘야한다
- LangChain으로는 이런 작업을 손쉽게 가능

### LangChain으로 구현할 RAG 시스템 전체 프로세스

```
[RAG]
문서 형식 -> 임베딩 -> 벡터 스토어(DB) -> 리트리버(검색기)
```

### RAG 프로세스의 사전 단계 이해하기

- GPT에게 학습되지 않은 정보를 주게 되면 할루시네이션 발생
- RAG에서는 미리 참고할 정보(PDF, CSV 등)를 데이터베이스나 문서 형태로 저장
- 사용자가 질문을 입력하면 리트리버(retriever)가 해당 질문과 유사성이 높은 문서를 데이터베이스에서 찾아 반환
- 반환한 것을 프롬프트에 포함하여 LLM이 컨텍스트에서 필요한 내용을 검색해서 답변을 생성하도록 돕는다
- 데이터를 전부 가져오긴 무리니깐 관련성 있는 정보만 골라서 리트리버에 전달하는 것이 효율적, 이 과정이 RAG 프로세스의 **사전단계**
- 사전 단계에서는 데이터 소스를 벡터 스토어로 사용하여 **문서로드-텍스트 분할-임베딩-저장**, 네 단계 진행

1. 문서 로드(Document Load): 외부 데이터 소스에서 필요한 문서를 불러와서 초기 처리. 학생이 공부하기 전에 필요한 책을 여러권 가져오는 것과 비슷
2. 텍스트 분할(Text Splitter): 불러온 문서를 작은 청크(조각)로 나누어 처리 용이. 책을 읽기 쉽게 챕터나 섹션으로 나누는 것과 비슷
3. 임베딩(Embedding): 각 텍스트 청크를 벡터 형태로 변환하여 컴퓨터가 이해할 수 있도록 함. 자연어를 컴퓨터가 이해할 수 있는 수치로 변경하는 과정
4. 벡터 스토어 저장(Vector Store Save): 임베딩된 벡터를 벡터 스토어에 저장하여 빠른 검색과 유사도 계산이 가능하도록 함. 요약된 키워드를 색인으로 뽑아서 나중에 빠르게 찾을 수 있게 정리하는 과정

- 즉, 문서 텍스트를 청크 단위로 나누어서 벡터로 임베딩(변환)한 뒤에 제일 일치하는 부분을 찾는다
- **청크 오버랩**: 분활된 청크 끝부분에서 맥락이 이어질 수 있도록 일부를 겹쳐서 분할
- 매번 텍스트 청크를 임베딩하면 비용이 들기에 벡터 스토어에 저장해두고, 쿼리 요청할 때마다 가져다 쓴다
- 문서를 로드하는 것에서 시작하여 임베딩하고 유사도 계산을 위한 벡터 값을 저장하는 과정을 **전처리 과정**

**RAG 프로세스의 실행 단계 이해하기**

- 사전 단계 이후의 실행 단계

5. 리트리버: 질문이 주어지면, 이와 관련된 벡터를 벡터 스토어에서 검색하여 관련 문서 청크를 찾아 반환. 질문에 가장 잘 맞는 책의 Chapter를 찾는 것과 유사
6. 프롬프트: 검색된 정보를 바탕으로 언어 모델을 위한 질문 구성. 정보를 바탕으로 어떻게 질문할지 결정하는 과정
7. LLM: 구성된 프롬프트를 사용하여 언어 모델이 답변을 생성. 수집된 정보를 바탕으로 보고서를 작성하는 학생과 같다
8. 체인 생성: 이전의 모든 과정을 하나의 파이프라인으로 묶어 주는 체인 생성


- **리트리버** 단계에서는 벡터 데이터베이스에서 사용자 질문과 관련된 문서를 검색하는 핵심 과정 
  - k 값을 설정하여 선택할 청크의 수 조절 가능
  - 리트리버의 성능은 전체 시스템의 응답 품질과 직결
- **체인 생성** 단계에서는 앞서 설명한 모든 단계를 하나의 파이프라인으로 묶어주는 역할
  - 사용자가 질문을 입력하면, 리트리버가 관련 문서를 검색하고, 프롬프트를 구성하여 LLM이 답변을 생성하는 전체 과정을 자동화
  - 이를 통해 사용자는 복잡한 내부 과정을 신경쓰지 않고도 RAG 시스템을 쉽게 활용 가능
 