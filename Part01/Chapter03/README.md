# Chapter03 LLM 기본 용어

## 1. Jupyter Notebook 사용법

- 커맨드 모드 단축키

```
D 두번 누르기: 쉘 삭제
Z: 삭제한 쉘 복구
ESC: 커서 모드 -> 커맨드 모드
A: 위에 셀 추가
B: 아래에 셀 추가
M: 마크다운 셀로 변경
Y: 코드 셀로 변경
Shift + Enter: 현재 셀 실행 후 아래로 이동
```

- 마크다운 문법은 GPT 같은 AI 모델이 더 잘 이해하는 문서를 만들기에도 용이

## 토큰, 토큰 계산기, 모델별 토큰 비용

- **토큰**은 자연어 처리에서 텍스트를 처리하기 위해 나눈 작은 단위
- 텍스트를 토큰으로 나누는 과정을 **토큰화**

**토큰화 방식**

- 토큰화는 문자 기반 토큰화와 단어 기반 토큰화 두가지로 나뉘어짐
- 문자 기반 토큰화: 텍스트를 개별 문자로 쪼개어 각 문자를 하나의 토큰으로 취급
- 단어 기반 토큰화: 텍스트를 단어 단위로 쪼개어 각 단어를 하나의 토큰으로 취급

```
[문자 기반]
hello -> h, e, l, l, o (5 tokens)

[단어 기반]
hello,world! -> "hello",  ",",  "world", "!" (4 tokens)
```

- LLM은 먼저 나올 토큰을 생성하면 그 다음 나올 토큰을 확률적으로 계산하여 생성하는 로직
- 문자 기반은 토큰이 너무 많아 예측하기 힘들고, 단어 기반은 모든 단어를 포함한 사전을 만들어야하기에 비효율적
- 이를 보완하기 위한 방법으로 단어 전체를 토큰화하는 대신 서브워드를 활용하는 **서브워드 기반 토큰화**
- 서브워드란 단어보다 작은 단위로, 접두사, 접미사, 어근 등이 해당
- 대표적인 방법으로 **바이트 페어 인코딩(BPE; Byte Pair Encoding)**

```
unhappiness -> un + happiness으로 토큰 분리
단어를 더 작은 단어로 쪼갠 후, 자주 등장하는 단위를 빈도에 따라 묶어 나간다
처음에는 u, n, h, a 등의 문자 단위로 나누고, un, nh, ha 등으로 각 문자 쌍을 묶어 빈도수 계산
자주 함께 등장하는 단어 쌍을 하나의 토큰으로 결합
빈도가 높은 un이 하나의 토큰으로 합쳐지고, happiness 같은 단어도 빈도가 높으므로 하나의 토큰으로 처리
```

- 토큰은 모델이 텍스트를 이해하고 처리하는데 매우 중요한 역할
- 텍스트를 토큰 단위로 나눈 후 숫자로 변환하는 과정. 이 숫자 변환은 좌표계 형식으로 표현되는데 이를 **임베딩 벡터**라고 함

```
[단어 기반 토큰화]
"Machine learning" -> ["Machine", "learning"]

[서브워드 기반 토큰화]
"Machine learning" -> ["Ma", "chine", "learn", "ing"]
```

### 토큰 사용량 계산하기

- GPT 모델에서는 입력 토큰과 출력 토큰의 합이 모두 토큰 수 계산에 포함
- 같은 의미의 텍스트라도 한글이 영어보다 토큰을 좀 더 소모. 그래도 최근에는 한글 토큰화도 많이 개선되어서 비슷해짐
- **Tiktok Tokenizer**: OpenAI 모델에서 사용하는 토큰을 계산하고 시각화하는 도구
  - 입력된 텍스트를 토큰화하여 얼마나 많은 토큰이 사용되는지를 정확하게 보여준다

```
System: GPT 모델이 내부적으로 사용하는 설정이나 사전 설정된 시스템 프롬프트
User: 사용자가 입력하는 질문이나 요청. 사용자는 이 부분에 질문이나 명령 작성
Assistant: GPT 모델이 생성하는 응답. 시스템 프롬프트와 사용자 입력을 기반으로 모델이 문맥을 이해하고 적절한 응답 제공

Show Whitespace 체크 박스: 옵션 활성화하면 공백이 어떻게 토큰으로 계산되는지를 시각적으로 확인 가능
```

## 3. 모델의 입출력과 컨텍스트 윈도우

- **컨텍스트 윈도우**: 모델이 한번에 처리하고 이해할 수 있는 전체 텍스트의 문맥 범위
  - LLM이 한번에 처리할 수 있는 입력과 출력의 총합 토큰 수를 **컨텍스트 길이**
- **max_tokens**: 모델이 생성할 수 있는 최대 출력 토큰 수
  - 모델이 응답으로 생성 가능한 최대 텍스트 길이 결정
- 컨텍스트 길이가 입력과 출력을 합친 것이기 때문에, 출력이 길어질 수록 입력으로 넣을 수 있는 텍스트 양이 줄어듬
- GPT-4 모델은 긴 문서를 다룰 수 있지만 max_tokens 출력 길이 제한 때문에 답변의 길이는 토큰 4,096개 이상을 넘길 수 없다
- 입력과 출력에 따른 비용 차이도 있는데, 출력 토큰의 비용이 3배 더 비쌈
  - 요약이나 질의 응답처럼 입력이 많고 답변이 간단한 작업은 상대적으로 저렴하지만, 기획안 작성이나 마케팅 시안 작성 같은 출력이 길어지는 작업은 비용이 많이 든다
- 입력되는 문서의 양이 많을 수록 토큰 비용이 증가하며, 입력 텍스트의 양을 신중하게 결정해야한다
- 하지만 컨텍스트 윈도우가 커졌다고 해도 RAG 시스템이 불필요하다는 얘기는 X. 필요한 부분만 발췌해 사용하는 것이 비용과 성능 면에서 더 효율적이라는 말